{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Includes plus re for string handling and nltk for simple language processing\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pre_process():\n",
    "    \"\"\"\n",
    "    This class is designed to take a filename linking to a text file and return a cleaned list for processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"\n",
    "        Take in a filename as a string to work on.\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        Get our data from some txt file. This will be split by word to be.\n",
    "        \"\"\"\n",
    "        # Load our txt file and split it by word\n",
    "        self.data = open(self.filename).read().split()\n",
    "        return self.data\n",
    "    \n",
    "    def seperate_speeches(self):\n",
    "        \"\"\"\n",
    "        This splits our speeches into seperate lists depending on who is speaking.\n",
    "        If new speeches are included you may input the names of the moderaters and speakers.\n",
    "        It returns a list of three lists with all speaking points for each speaker contatenated.\n",
    "        \"\"\"\n",
    "        debate = self.data\n",
    "        mod_list = []\n",
    "        can1_list = []\n",
    "        can2_list = []\n",
    "\n",
    "        i_new = -1\n",
    "        i_old = -1\n",
    "        while i_new >= -len(debate):\n",
    "            if (debate[i_new] == 'LEHRER:' or debate[i_new] == 'CROWLEY:' or debate[i_new] == 'SCHIEFFER:'\n",
    "                    or debate[i_new] == 'HOLT:' or debate[i_new] == 'RADDATZ:'or debate[i_new] == 'WALLACE:'):\n",
    "                mod_list += debate[i_new:]\n",
    "                del debate[i_new:i_old]\n",
    "                i_old = i_new\n",
    "\n",
    "            elif debate[i_new] == 'OBAMA:' or debate[i_new] == 'CLINTON:':\n",
    "                can1_list += debate[i_new:]\n",
    "                del debate[i_new:i_old]\n",
    "                i_old = i_new\n",
    "\n",
    "            elif debate[i_new] == 'ROMNEY:' or debate[i_new] == 'TRUMP:':\n",
    "                can2_list += debate[i_new:]\n",
    "                del debate[i_new:i_old]\n",
    "                i_old = i_new\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "            i_new -= 1\n",
    "        \n",
    "        speeches = (can1_list, can2_list)\n",
    "        \n",
    "        can_aslists = []\n",
    "        for j in range(2):\n",
    "            can_aslists.append(' '.join(speeches[j]))\n",
    "        return can_aslists\n",
    "    \n",
    "    def clean(self, data):\n",
    "        \"\"\"\n",
    "        Takes in our data and strips all unnecessary characters and names for efficiency.\n",
    "        This step is not required for basic sentiment analysis in general, but is for more\n",
    "        complex analysis.\n",
    "        \"\"\"\n",
    "        # Characters to remove\n",
    "        remove = re.compile(r\"[\\'\\\"\\\\\\!\\,\\/\\;\\{\\}\\[\\.\\]]\")\n",
    "        remove_names = re.compile(r'(ROMNEY:) (OBAMA:) (LEHRER:) (CLINTON:) (SCHIEFFER:) (CRAWLEY:) (TRUMP:)')\n",
    "        # List to store clean speeches\n",
    "        removed_names = []\n",
    "        clean_data = []\n",
    "        # Loop over each speech and remove all regex excluded characters defined above\n",
    "        for speech in data:\n",
    "            remove_char = remove.sub(\"\", speech)\n",
    "            removed_names.append(remove_char)\n",
    "        for speech in removed_names:\n",
    "            clean_speech = remove_names.sub(\"\", speech)\n",
    "            clean_data.append(clean_speech)\n",
    "\n",
    "        return clean_data\n",
    "    \n",
    "    def remove_stop_words(self, clean_speeches):\n",
    "        \"\"\"\n",
    "        Use nltk stopwords to remove a subset of words from each space.\n",
    "        The removed words take away very little context and so don't affect analysis, but \n",
    "        is much quicker to sort through for analysis.\n",
    "        \"\"\"\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        removed_words_speeches = []\n",
    "        for speech in clean_speeches:\n",
    "            removed_words_speeches.append(' '.join([word for word in speech.split() if word not in stop_words]))\n",
    "        return removed_words_speeches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_analysis():\n",
    "    \"\"\"\n",
    "    This class loads in our positive and negative words for our sentiment analysis and counts\n",
    "    the good and bad sentiment words for each speech given.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Take in our data and store it.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "    \n",
    "    def read_good_bad(self, pos, neg):\n",
    "        \"\"\"\n",
    "        Create a list containing the words with positive and negative sentiment.\n",
    "        \"\"\"\n",
    "        self.pos = np.loadtxt(pos, dtype = str,encoding=\"ISO-8859-1\")\n",
    "        self.neg = np.loadtxt(neg, dtype=str ,encoding=\"ISO-8859-1\")\n",
    "        return [list(self.pos), list(self.neg)]\n",
    "\n",
    "    def sentiment_count(self, speech):\n",
    "        \"\"\"\n",
    "        Count the good and bad sentiment words for each speech and return them as a tuple.\n",
    "        \"\"\"\n",
    "        good_count = [x for x in speech if x in good_bad[0]]\n",
    "        bad_count = [x for x in speech if x in good_bad[1]]\n",
    "        return (good_count, bad_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our data locations\n",
    "\n",
    "Speech_list = ['/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2012/Obama_Romeny_1.txt',\n",
    "                '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2012/Obama_Romeny_2.txt',\n",
    "                '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2012/Obama_Romeny_3.txt',\n",
    "                '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2016/Clinton_Trump_1.txt',\n",
    "                '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2016/Clinton_Trump_2.txt',\n",
    "                '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2016/Clinton_Trump_3.txt']\n",
    "\n",
    "good_list = '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Pre-Processing and Sentiment Analysis/opinion-lexicon-English/positive-words.txt'\n",
    "bad_list = '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Pre-Processing and Sentiment Analysis/opinion-lexicon-English/negative-words.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize temporary values for storing, indexing, and date placement.\n",
    "output = []\n",
    "i = 0\n",
    "j = 0\n",
    "Debate_Date = ['10/3/2012', '10/16/2012', '10/22/2012', '9/26/2016', '10/9/2016', '10/19/2016']\n",
    "\n",
    "# Loop over all speeches in the list above\n",
    "for val in Speech_list:\n",
    "    \n",
    "    # grab the speech\n",
    "    data = pre_process(val)\n",
    "\n",
    "    # extract the speeches data\n",
    "    y = data.get_data()\n",
    "\n",
    "    # seperate the speeches by speaker\n",
    "    r = data.seperate_speeches()\n",
    "\n",
    "    # clean each speech\n",
    "    x = data.clean(r)\n",
    "\n",
    "    # remove the stop words\n",
    "    z = data.remove_stop_words(x)\n",
    "    \n",
    "    # initialize our analysis on the cleaned speech\n",
    "    SA = sentiment_analysis(z)\n",
    "    \n",
    "    # grab our positive and negative sentiment list\n",
    "    good_bad = SA.read_good_bad(good_list, bad_list)\n",
    "    \n",
    "    # loop over each speaker in the speech\n",
    "    for q in range(len(z)):\n",
    "        # finds out who is speaking based on indexing\n",
    "        if i==0 or i==1 or i==2:\n",
    "            # Democrates come first \n",
    "            if q==0:\n",
    "                Candidate_Name = 'Barack Obama'\n",
    "                Affiliation = 'D'\n",
    "                Election_Year = 2012\n",
    "            else:\n",
    "                Candidate_Name = 'Mitt Romney'\n",
    "                Affiliation = 'R'\n",
    "                Election_Year = 2012\n",
    "        if i==3 or i==4 or i==5:\n",
    "            if q==0:\n",
    "                Candidate_Name = 'Hillary Clinton'\n",
    "                Affiliation = 'D'\n",
    "                Election_Year = 2016\n",
    "            else:\n",
    "                Candidate_Name = 'Donald Trump'\n",
    "                Affiliation = 'R'\n",
    "                Election_Year = 2016\n",
    "        # split our speeches by word to perform analysis\n",
    "        y = SA.sentiment_count(z[q].split())\n",
    "        good = len(y[0])\n",
    "        bad = len(y[1])\n",
    "        \n",
    "        # return the prefered data.\n",
    "        output.append([Election_Year, Debate_Date[j], Candidate_Name, Affiliation, good, bad, good+bad])\n",
    "    i += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create it as a pandas Data Frame\n",
    "sentiment_12_16 = pd.DataFrame(output, columns=['Election_Year', 'Debate_Date', 'Candidate_Name', 'Affiliation', 'Positive_Words', 'Negative_Words', 'Total_Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export it as a csv\n",
    "sentiment_12_16.to_csv('SentimentData_12-16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
