{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Includes\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pre_process():\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"Get our data from some csv file. The file should be a csv.\n",
    "        Each cell should be a string and each string should be one entire speech.\n",
    "        Must be run prior to clean().\"\"\"\n",
    "        # Load our csv as strings\n",
    "        self.data = open(self.filename).read().split()\n",
    "        return self.data\n",
    "    \n",
    "    def seperate_speeches(self):\n",
    "        debate = self.data\n",
    "        mod_list = []\n",
    "        can1_list = []\n",
    "        can2_list = []\n",
    "\n",
    "        i_new = -1\n",
    "        i_old = -1\n",
    "        while i_new >= -len(debate):\n",
    "            if debate[i_new] == 'LEHRER:' or debate[i_new] == 'CROWLEY:' or debate[i_new] == 'SCHIEFFER:' or debate[i_new] == 'HOLT:' or debate[i_new] == 'RADDATZ:'or debate[i_new] == 'WALLACE:':\n",
    "                mod_list += debate[i_new:]\n",
    "                del debate[i_new:i_old]\n",
    "                i_old = i_new\n",
    "\n",
    "            elif debate[i_new] == 'OBAMA:' or debate[i_new] == 'CLINTON:':\n",
    "                can1_list += debate[i_new:]\n",
    "                del debate[i_new:i_old]\n",
    "                i_old = i_new\n",
    "\n",
    "            elif debate[i_new] == 'ROMNEY:' or debate[i_new] == 'TRUMP:':\n",
    "                can2_list += debate[i_new:]\n",
    "                del debate[i_new:i_old]\n",
    "                i_old = i_new\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "            i_new -= 1\n",
    "        \n",
    "        speeches = (can1_list, can2_list)\n",
    "        \n",
    "        can_aslists = []\n",
    "        for j in range(2):\n",
    "            can_aslists.append(' '.join(speeches[j]))\n",
    "        return can_aslists\n",
    "    \n",
    "    def clean(self, data):\n",
    "        \"\"\"Takes in our data and strips all unnecessary characters.\"\"\"\n",
    "        # Characters to remove\n",
    "        remove = re.compile(r\"[\\'\\\"\\\\\\!\\,\\/\\;\\{\\}\\[\\.\\]]\")\n",
    "        remove_names = re.compile(r'(ROMNEY:) (OBAMA:) (LEHRER:) (CLINTON:) (SCHIEFFER:) (CRAWLEY:) ()')\n",
    "        # List to store clean speeches\n",
    "        removed_names = []\n",
    "        clean_data = []\n",
    "        # Loop over each speech and remove all regex excluded characters defined above in remove\n",
    "        for speech in data:\n",
    "            remove_char = remove.sub(\"\", speech)\n",
    "            removed_names.append(remove_char)\n",
    "        for speech in removed_names:\n",
    "            clean_speech = remove_names.sub(\"\", speech)\n",
    "            clean_data.append(clean_speech)\n",
    "\n",
    "        return clean_data\n",
    "    \n",
    "    def remove_stop_words(self, clean_speeches):\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        removed_words_speeches = []\n",
    "        for speech in clean_speeches:\n",
    "            removed_words_speeches.append(' '.join([word for word in speech.split() if word not in stop_words]))\n",
    "        return removed_words_speeches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_analysis():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def read_good_bad(self, pos, neg):\n",
    "        self.pos = np.loadtxt(pos, dtype = str,encoding=\"ISO-8859-1\")\n",
    "        self.neg = np.loadtxt(neg, dtype=str ,encoding=\"ISO-8859-1\")\n",
    "        return [list(self.pos), list(self.neg)]\n",
    "\n",
    "    def sentiment_count(self, speech):\n",
    "        good_count = [x for x in speech if x in good_bad[0]]\n",
    "        bad_count = [x for x in speech if x in good_bad[1]]\n",
    "        return (good_count, bad_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Speech_list = ['/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2012/Obama_Romeny_1.txt',\n",
    "                '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2012/Obama_Romeny_2.txt',\n",
    "                '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2012/Obama_Romeny_3.txt',\n",
    "                '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2016/Clinton_Trump_1.txt',\n",
    "                '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2016/Clinton_Trump_2.txt',\n",
    "                '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Debate_Scrape/Debates/2016/Clinton_Trump_3.txt']\n",
    "\n",
    "good_list = '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Pre-Processing and Sentiment Analysis/opinion-lexicon-English/positive-words.txt'\n",
    "bad_list = '/Users/rileylatham/Downloads/CMSE202/CMSE202_Final/Pre-Processing and Sentiment Analysis/opinion-lexicon-English/negative-words.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OR_GB = []\n",
    "i = 0\n",
    "j = 0\n",
    "for val in Speech_list:\n",
    "    \n",
    "    data = pre_process(val)\n",
    "\n",
    "    y = data.get_data()\n",
    "\n",
    "    r = data.seperate_speeches()\n",
    "\n",
    "    x = data.clean(r)\n",
    "\n",
    "    z = data.remove_stop_words(x)\n",
    "    \n",
    "    SA = sentiment_analysis(z)\n",
    "    \n",
    "    good_bad = SA.read_good_bad(good_list, bad_list)\n",
    "    \n",
    "    for q in range(len(z)):\n",
    "        if i==0 or i==1 or i==2:\n",
    "            if q==0:\n",
    "                name = 'Obama'\n",
    "            else:\n",
    "                name = 'Romney'\n",
    "        if i==3 or i==4 or i==5:\n",
    "            if q==0:\n",
    "                name = 'Clinton'\n",
    "            else:\n",
    "                name = 'Trump'\n",
    "        \n",
    "        y = SA.sentiment_count(z[q].split())\n",
    "        good = len(y[0])\n",
    "        bad = len(y[1])\n",
    "        OR_GB.append([name, good, bad])\n",
    "    i += 1\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Obama', 3824, 1438], ['Romney', 4307, 1641], ['Obama', 3240, 1748], ['Romney', 4628, 2448], ['Obama', 4569, 2393], ['Romney', 4368, 2255], ['Clinton', 3704, 3388], ['Trump', 4637, 4225], ['Clinton', 2646, 2416], ['Trump', 3571, 3353], ['Clinton', 5446, 4316], ['Trump', 5720, 4439]]\n"
     ]
    }
   ],
   "source": [
    "print(OR_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
